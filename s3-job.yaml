apiVersion: v1
kind: ConfigMap
metadata:
  name: ray-job-code-sample
  namespace: default
data:
  requirements.txt: |
    boto3
    botocore
    
  entrypoint.sh: |
    #!/bin/bash
    pip install -r /home/ray/samples/requirements.txt
    python /home/ray/samples/data_loader.py

  data_loader.py: |
    import ray
    from qdrant_client import QdrantClient
    from sentence_transformers import SentenceTransformer
    from typing import Dict, List
    import os
    import boto3
    import json

    # Get S3 configuration from environment variables
    S3_BUCKET = os.environ.get('S3_BUCKET')
    S3_KEY = os.environ.get('S3_KEY')
    AWS_REGION = os.environ.get('AWS_DEFAULT_REGION', 'us-west-2')

    @ray.remote
    class QdrantIngester:
        def __init__(self):
            self.qdrant_client = QdrantClient(
                "qdrant",
                port=6333
            )
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

        def create_collection(self):
            try:
                self.qdrant_client.recreate_collection(
                    collection_name="knowledge_base",
                    vectors_config={
                        "size": 384,
                        "distance": "Cosine"
                    }
                )
                print("Collection created successfully")
            except Exception as e:
                print(f"Error creating collection: {e}")

        def process_batch(self, batch: List[Dict]) -> int:
            try:
                vectors = []
                for item in batch:
                    embedding = self.embedding_model.encode(item["text"])
                    vector = {
                        "id": abs(hash(item["text"])),
                        "vector": embedding.tolist(),
                        "payload": item
                    }
                    vectors.append(vector)

                self.qdrant_client.upsert(
                    collection_name="knowledge_base",
                    points=vectors
                )
                print(f"Processed batch of {len(vectors)} items")
                return len(vectors)
            except Exception as e:
                print(f"Error processing batch: {e}")
                return 0

    def load_data_from_s3():
        try:
            if not S3_BUCKET or not S3_KEY:
                raise ValueError("S3_BUCKET and S3_KEY environment variables must be set")

            print(f"Loading data from s3://{S3_BUCKET}/{S3_KEY}")
            s3_client = boto3.client('s3', region_name=AWS_REGION)
            response = s3_client.get_object(
                Bucket=S3_BUCKET,
                Key=S3_KEY
            )
            
            data = []
            content = response['Body'].read().decode('utf-8')
            for line in content.split('\n'):
                if line.strip():
                    item = json.loads(line)
                    data.append({
                        "text": item.get("text", ""),
                        "category": item.get("category", "unknown"),
                        "source": "electronics_dataset"
                    })
            return data
        except Exception as e:
            print(f"Error loading data from S3: {e}")
            return []

    def main():
        print("Initializing Ray...")
        ray.init()
        
        print("Loading data from S3...")
        data = load_data_from_s3()
        print(f"Loaded {len(data)} items")
        
        print("Creating QdrantIngester...")
        ingester = QdrantIngester.remote()
        
        print("Creating collection...")
        ray.get(ingester.create_collection.remote())
        
        batch_size = 100
        batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]
        
        print(f"Processing {len(batches)} batches...")
        futures = [ingester.process_batch.remote(batch) for batch in batches]
        results = ray.get(futures)
        
        total_processed = sum(results)
        print(f"Total records processed: {total_processed}")

    if __name__ == "__main__":
        main()

---
apiVersion: ray.io/v1
kind: RayJob
metadata:
  name: qdrant-loader-job
  namespace: default
spec:
  entrypoint: bash /home/ray/samples/entrypoint.sh
  shutdownAfterJobFinishes: false
  rayClusterSpec:
    rayVersion: '2.43.0'
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: '1'
      template:
        spec:
          serviceAccountName: ray-service-account
          containers:
            - name: ray-head
              image: public.ecr.aws/j5k9c6o6/rag:latest
              env:
                - name: PYTHONPATH
                  value: "/home/ray/samples"
                - name: AWS_DEFAULT_REGION
                  value: "us-west-2"
                - name: S3_BUCKET
                  value: "rag-sample-data-test"
                - name: S3_KEY
                  value: "electronics.jsonl"
                - name: PIP_DISABLE_PIP_VERSION_CHECK
                  value: "1"
                - name: PIP_NO_CACHE_DIR
                  value: "1"
              volumeMounts:
                - mountPath: /home/ray/samples
                  name: code-sample
              resources:
                limits:
                  cpu: "8"
                  memory: "32Gi"
                requests:
                  cpu: "8"
                  memory: "32Gi"
          volumes:
            - name: code-sample
              configMap:
                name: ray-job-code-sample
                defaultMode: 0777  # Make the entrypoint.sh executable
          nodeSelector:
            NodeGroupType: x86-cpu-karpenter
            type: karpenter
    workerGroupSpecs:
    - groupName: cpu-workers
      replicas: 1
      minReplicas: 1
      maxReplicas: 4
      rayStartParams: {}
      template:
        spec:
          serviceAccountName: ray-service-account
          containers:
          - name: ray-worker
            image: public.ecr.aws/j5k9c6o6/rag:latest
            env:
              - name: PYTHONPATH
                value: "/home/ray/samples"
              - name: AWS_DEFAULT_REGION
                value: "us-west-2"
              - name: S3_BUCKET
                value: "rag-sample-data-test"
              - name: S3_KEY
                value: "electronics.jsonl"
              - name: PIP_DISABLE_PIP_VERSION_CHECK
                value: "1"
              - name: PIP_NO_CACHE_DIR
                value: "1"
            volumeMounts:
              - mountPath: /home/ray/samples
                name: code-sample
            resources:
              limits:
                cpu: "6"
                memory: "28Gi"
                nvidia.com/gpu: "1"
              requests:
                cpu: "6"
                memory: "28Gi"
                nvidia.com/gpu: "1"
          volumes:
            - name: code-sample
              configMap:
                name: ray-job-code-sample
                defaultMode: 0777
          nodeSelector:
              NodeGroupType: g5-gpu-karpenter
              type: karpenter
          tolerations:
            - key: "nvidia.com/gpu"
              operator: "Exists"
              effect: "NoSchedule"
